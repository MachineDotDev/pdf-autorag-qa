name: PDF Q&A AutoRAG Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'pdfs/**'
      - 'qa_extraction_lib/**'
      - 'cli_pdf_qa.py'
      - 'domain_eval_gpu.py'
  workflow_dispatch:
    inputs:
      input_file:
        description: 'Input PDF file path'
        required: true
        default: 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf'
      model_name:
        description: 'Hugging Face model name'
        required: false
        default: 'meta-llama/Meta-Llama-3-8B-Instruct'
      chunk_size:
        description: 'Chunk size in words'
        required: false
        default: '800'
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '8'
      use_quantization:
        description: 'Use 4-bit quantization to save GPU memory'
        required: false
        default: 'false'
      max_questions:
        description: 'Maximum number of evaluation questions'
        required: false
        default: '15'
      enable_bert_score:
        description: 'Enable BERT-score evaluation'
        required: false
        default: 'true'
      top_k_selection:
        description: 'Top K Q&A pairs to select for AutoRAG evaluation'
        required: false
        default: '50'

jobs:
  # Serial PDF Q&A Generation + AutoRAG Pipeline (All permutations in one job)
  pdf-qa-autorag-serial:
    runs-on:
      - machine
      - gpu=l40s
      - cpu=8
      - ram=64
      - architecture=x64
      - tenancy=spot
    timeout-minutes: 360
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_HUB_ENABLE_HF_TRANSFER: 1
      HF_HUB_DOWNLOAD_TIMEOUT: 120
      CUDA_LAUNCH_BLOCKING: 1
      TORCH_USE_CUDA_DSA: 1
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.local/bin" >> $GITHUB_PATH
        
    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo "$HOME/.local/bin" >> $GITHUB_PATH
        
    - name: Configure Poetry to use uv
      run: |
        poetry self add poetry-plugin-up
        poetry config installer.parallel true
        poetry config virtualenvs.create false
        
    - name: Install dependencies with Poetry + uv
      run: |
        # Use uv for faster installation
        export UV_SYSTEM_PYTHON=1
        poetry install --with gpu --no-dev
        
        # Verify critical dependencies
        python -c "import sentence_transformers; print('‚úÖ sentence-transformers installed')"
        python -c "import faiss; print('‚úÖ faiss installed')"
        python -c "import torch; print('‚úÖ torch installed')"
    
    - name: Login to Hugging Face
      run: |
        python -c "from huggingface_hub import login; login('${{ secrets.HF_TOKEN }}')"

    - name: Create output directories
      run: |
        mkdir -p outputs rag_input rag_store autorag_results
        echo "üìÅ Created pipeline directories"
        echo "üöÄ AutoRAG Pipeline Starting..."

    # Run all 9 permutations sequentially (model loads once and stays in memory)
    - name: "Q&A Generation - Basic √ó High Creativity"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Basic √ó Balanced"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Basic √ó Conservative"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó High Creativity"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó Balanced"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó Conservative"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó High Creativity"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó Balanced"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó Conservative"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: Q&A Generation Summary
      run: |
        echo "üéØ ============================================"
        echo "   Q&A GENERATION COMPLETED - ALL PERMUTATIONS"
        echo "=============================================="
        
        echo "üìä Generated Q&A Files:"
        find outputs/ -name "*.jsonl" -type f -exec basename {} \; | sort
        
        echo ""
        echo "üìà Q&A Pairs Summary:"
        find outputs/ -name "*.jsonl" -type f -exec wc -l {} + | tail -1
        
        echo ""
        echo "üöÄ Starting AutoRAG pipeline with all permutation results..."

    # Continue with AutoRAG pipeline using all generated results
    - name: Create AutoRAG Input Directory
      run: |
        echo "üìÅ Preparing AutoRAG input from all permutations"
        
        # List all generated Q&A files
        echo "üì• Q&A Generation Results:"
        find outputs/ -name "*.jsonl" -type f | head -20
        
        # Count total Q&A pairs across all matrix combinations
        echo "üìä Q&A Pairs Summary:"
        find outputs/ -name "*.jsonl" -type f -exec wc -l {} + | tail -1
    
    - name: Select Top K Q&A Pairs
      run: |
        python qa_pair_selector.py \
          --qa-artifacts-dir outputs \
          --output-dir rag_input \
          --top-k ${{ github.event.inputs.top_k_selection || '50' }} \
          --verbose
    
    - name: Build Q&A Vector Store
      run: |
        python qa_faiss_builder.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --output-dir rag_store \
          --embedding-model all-MiniLM-L6-v2
    
    - name: Run Q&A AutoRAG Evaluation
      run: |
        # Use GPU index if available, fallback to CPU
        if [ -f "rag_store/qa_faiss_index_gpu.bin" ]; then
          echo "üöÄ Using GPU-optimized FAISS index"
          FAISS_INDEX="rag_store/qa_faiss_index_gpu.bin"
        else
          echo "‚ö†Ô∏è GPU index not found, using CPU index"
          FAISS_INDEX="rag_store/qa_faiss_index_cpu.bin"
        fi
        
        python qa_autorag_evaluator.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --qa-faiss-index "$FAISS_INDEX" \
          --qa-metadata rag_store/qa_metadata.json \
          --output-dir autorag_results \
          --model-name "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --embedding-model all-MiniLM-L6-v2 \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantization' || '' }} \
          --verbose
    
    - name: Generate High-Quality Training Dataset
      run: |
        python training_dataset_generator.py \
          --evaluation-results autorag_results/qa_rag_evaluation_report.json \
          --output-dir autorag_results \
          --min-similarity 0.3 \
          --min-length-ratio 0.5 \
          --max-length-ratio 2.0 \
          --min-answer-length 20 \
          --verbose

    - name: Run Audio Equipment Domain Specificity Evaluation
      run: |
        python domain_eval_gpu.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --results-dir outputs \
          --config audio_equipment_domain_questions.json \
          --max-questions ${{ github.event.inputs.max_questions || '15' }} \
          ${{ github.event.inputs.enable_bert_score == 'false' && '--no-bert-score' || '' }} \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }}

    # Single artifact upload at the end with all results
    - name: Upload Complete Pipeline Results
      uses: actions/upload-artifact@v4
      with:
        name: pdf-qa-autorag-serial-complete
        path: |
          outputs/
          rag_input/
          rag_store/
          autorag_results/
          domain_eval_results.csv
          domain_eval_analysis.json
        retention-days: 30

    - name: Display Pipeline Summary
      run: |
        echo "üéØ ============================================"
        echo "   PDF Q&A AutoRAG Pipeline - FINAL RESULTS"
        echo "=============================================="
        
        if [ -f "autorag_results/final_evaluation_report.json" ]; then
          echo "‚úÖ AutoRAG Pipeline completed successfully"
          echo ""
          echo "üìä PIPELINE SUMMARY:"
          echo "==================="-debug
          python -c "
        import json
        with open('autorag_results/final_evaluation_report.json', 'r') as f:
            report = json.load(f)
        
        summary = report['pipeline_summary']
        print(f'Matrix Combinations Tested: {summary[\"total_matrix_combinations\"]}')
        print(f'Q&A Pairs Selected: {summary[\"total_evaluated_pairs\"]}')
        print(f'Pairs RAG Evaluated: {summary[\"total_evaluated_pairs\"]}')
        print(f'High-Quality Training Pairs: {summary[\"high_quality_pairs\"]}')
        print(f'Quality Retention Rate: {summary[\"quality_retention_rate\"]:.1%}')
        print()
        
        if report['recommendations']['best_matrix_combinations']:
            print('üèÜ TOP PERFORMING COMBINATIONS:')
            for i, combo in enumerate(report['recommendations']['best_matrix_combinations'], 1):
                print(f'  {i}. {combo[\"combination\"]}: {combo[\"avg_quality\"]:.3f} avg quality ({combo[\"pair_count\"]} pairs)')
          "
        else
          echo "‚ùå AutoRAG pipeline failed"
        fi
        
        echo ""
        if [ -f "domain_eval_analysis.json" ]; then
          echo "üìà DOMAIN EVALUATION RESULTS:"
          echo "============================"
          python -c "
        import json
        with open('domain_eval_analysis.json', 'r') as f:
            analysis = json.load(f)
        
        print(f'Domain Evaluation Questions: {analysis[\"total_questions\"]}')
        print(f'Base Model Domain Relevance: {analysis[\"avg_base_domain_relevance\"]:.3f}')
        print(f'RAG Model Domain Relevance: {analysis[\"avg_rag_domain_relevance\"]:.3f}')
        
        if 'avg_base_bert_f1' in analysis:
            print(f'Base Model BERT F1: {analysis[\"avg_base_bert_f1\"]:.3f}')
            print(f'RAG Model BERT F1: {analysis[\"avg_rag_bert_f1\"]:.3f}')
          "
        fi
        
        echo ""
        echo "üöÄ READY FOR LOCAL MODEL TRAINING:"
        echo "=================================="
        echo "‚úÖ High-quality Q&A dataset: autorag_results/high_quality_pairs.jsonl"
        echo "‚úÖ FAISS vector store: rag_store/faiss_index.bin"  
        echo "‚úÖ Evaluation reports: autorag_results/final_evaluation_report.json"
        echo "‚úÖ Domain analysis: domain_eval_analysis.json"
        echo ""
        echo "üí° Next Steps:"
        echo "  1. Download artifacts for local training"
        echo "  2. Use high_quality_pairs.jsonl for fine-tuning"
        echo "  3. Deploy FAISS index for RAG frontend"
        echo "  4. Monitor training with evaluation reports"